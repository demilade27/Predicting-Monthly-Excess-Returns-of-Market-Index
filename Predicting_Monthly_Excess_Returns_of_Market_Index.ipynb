{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/demilade27/Predicting-Monthly-Excess-Returns-of-Market-Index/blob/main/Predicting_Monthly_Excess_Returns_of_Market_Index.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EgeOWEzKxPYA"
   },
   "source": [
    "# Pre-requisite\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEOjCULwX4Vb"
   },
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CFIzTHKLdi2x",
    "outputId": "dcab9fb5-c8aa-474c-ac94-35c66ccfbe44"
   },
   "outputs": [],
   "source": [
    "# Comment out the pip requirement \n",
    "# !pip install -r requirements.txt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, LassoCV, Lasso\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score, root_mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf, plot_ccf\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from typing import Final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N22HDiDOnKav"
   },
   "source": [
    "## Load The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "HcO3eLKlonql"
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv('https://raw.githubusercontent.com/demilade27/Predicting-Monthly-Excess-Returns-of-Market-Index/d2f2cb8478612fa4e8fd4e87628375d44f6cb72e/data.csv')\n",
    "# when offline \n",
    "df = pd.read_csv(\"/Users/demiladepopoola/Development/Branch /Predicting-Monthly-Excess-Returns-of-Market-Index/data.csv\")\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.set_index('date', inplace=True)\n",
    "DF_FINAL: Final = df  # This is the original data set that would not change so that we can use the original data for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sIt3PcGrmrb0"
   },
   "source": [
    "# Analyse the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "DxiwGi3OpMvO",
    "outputId": "498f27e5-276c-4ce5-df6b-98420fa32961"
   },
   "outputs": [],
   "source": [
    "DF_FINAL.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for null or zero values\n",
    "*Analysis*\n",
    "---\n",
    "There are no null values in the dataset\n",
    "Analysing the dataset values there are zero values\n",
    "* INFL: There are 239 zero values showing signs of Deflationary Stagnation\n",
    "* DE:\n",
    "* LTR:\n",
    "* TMS:\n",
    "* DFR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nHeJDb2ZpNWe",
    "outputId": "0c343de5-a42b-4b2f-e5a0-1990d1fbcdac"
   },
   "outputs": [],
   "source": [
    "print(DF_FINAL.isnull().sum())\n",
    "print(DF_FINAL.duplicated().sum())\n",
    "print((DF_FINAL == 0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse Data Skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_FINAL.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-f46ZB9nUjVB"
   },
   "source": [
    "\n",
    "## Data Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation analysis of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 717
    },
    "id": "V8aGqNGKkeIH",
    "outputId": "71d06c2a-848e-43f9-da03-53113caf7238"
   },
   "outputs": [],
   "source": [
    "correlation_matrix=DF_FINAL .corr()\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(correlation_matrix,annot=True,fmt=\".2f\",cmap=\"coolwarm\")\n",
    "plt.title('Correlation Heatmap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation analysis with R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-pFT80uH4M9q",
    "outputId": "e17154a9-b3b6-4e63-addc-5235bb4094d2"
   },
   "outputs": [],
   "source": [
    "target_correlation = correlation_matrix[['R']]\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(target_correlation,annot=True,fmt=\".2f\",cmap=\"coolwarm\")\n",
    "plt.title('Correlation Heatmap with R')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "j3LxX_9Ukmj5",
    "outputId": "c04b37c2-ebce-4940-ca19-119c2f7849fb"
   },
   "outputs": [],
   "source": [
    "sns.pairplot(DF_FINAL,y_vars=['R'],x_vars=DF_FINAL.select_dtypes(include='number').columns,kind='reg')\n",
    "plt.suptitle(\"Pairwise Scatterplots with Fitted Lines\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box Plot\n",
    "This is the analysis of the skewness of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "id": "Qn8rKJcuegJ4",
    "outputId": "4edbe25b-cea5-4fe2-9a8b-0f8d809a9aca"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "DF_FINAL.boxplot()\n",
    "plt.title('Box Plot of Training Data Features')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RcLOrUak5cj"
   },
   "source": [
    "### Autocorrelation and Partial Autocorrelation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12 Month lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 887
    },
    "collapsed": true,
    "id": "gef4IHGxYZj0",
    "outputId": "4273e9ea-b19d-4655-ccdb-8ca320129cbd"
   },
   "outputs": [],
   "source": [
    "plot_acf(DF_FINAL['mr'],lags=12)\n",
    "plot_pacf(DF_FINAL['mr'],lags=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 24 Month lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 887
    },
    "id": "J5comPvBixlD",
    "outputId": "004fb35b-023a-4f72-bea6-1a4ad6fc01bf"
   },
   "outputs": [],
   "source": [
    "plot_acf(DF_FINAL['mr'],lags=24)\n",
    "plot_pacf(DF_FINAL['mr'],lags=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpnLvYdZnKFY"
   },
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BvVcOuLKL_h_"
   },
   "source": [
    "## Data Spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date='2019-01-01'\n",
    "train_data=df[df.index <split_date]\n",
    "x_train=train_data.drop('R',axis=1)\n",
    "y_train=train_data[['R']]\n",
    "test_data=df[df.index >=split_date]\n",
    "x_test=test_data.drop('R',axis=1)\n",
    "y_test=test_data[['R']]\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHWTNYpO0TwG"
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving Averages and Rolling Volitility "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    x_train[f'mr_rolling_mean'] = x_train['mr'].rolling(window=3).mean()\n",
    "    x_train[f'mr_rolling_std'] = x_train['mr'].rolling(window=3).std()\n",
    "\n",
    "    x_test[f'mr_rolling_mean'] = x_test['mr'].rolling(window=3).mean()\n",
    "    x_test[f'mr_rolling_std'] = x_test['mr'].rolling(window=3).std()\n",
    "\n",
    "    x_train[f'svar_rolling_mean'] = x_train['svar'].rolling(window=3).mean()\n",
    "    x_train[f'svar_rolling_std'] = x_train['svar'].rolling(window=3).std()\n",
    "\n",
    "    x_test[f'svar_rolling_mean'] = x_test['svar'].rolling(window=3).mean()\n",
    "    x_test[f'svar_rolling_std'] = x_test['svar'].rolling(window=3).std()\n",
    "\n",
    "x_train['dfy_ma3'] = x_train['dfy'].rolling(window=3).mean()\n",
    "\n",
    "x_test['dfy_ma3'] = x_test['dfy'].rolling(window=3).mean()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    x_train[f'tbl_rolling_std'] = x_train['tbl'].rolling(window=3).std()\n",
    "    x_test[f'tbl_rolling_std'] = x_test['tbl'].rolling(window=3).std()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(x_train.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix of Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lagging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lagged variables\n",
    "for var in ['mr', 'dfr', 'infl', 'svar','ntis']:\n",
    "    for lag in range(1, 3):\n",
    "        x_train[f'{var}_lag{lag}'] = x_train[var].shift(lag)\n",
    "        x_test[f'{var}_lag{lag}'] = x_test[var].shift(lag)\n",
    "\n",
    "# Handle missing values\n",
    "x_train['b/m_lag1'] = x_train['b/m'].shift(1)\n",
    "x_test['b/m_lag1'] = x_test['b/m'].shift(1)\n",
    "x_train = x_train.fillna(method='bfill')\n",
    "x_test = x_test.fillna(method='bfill')\n",
    "x_train.dropna(inplace=True)\n",
    "\n",
    "x_test.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interaction terms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_pairs = [('tms', 'infl'), ('dp', 'dy'), ('dfy', 'tbl')]\n",
    "for var1, var2 in interaction_pairs:\n",
    "    x_train[f'{var1}_x_{var2}'] = x_train[var1] * x_train[var2]\n",
    "    x_test[f'{var1}_x_{var2}'] = x_test[var1] * x_test[var2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum and Volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### drop   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.drop('lty', axis=1) \n",
    "x_test = x_test.drop('lty', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.drop('dfy', axis=1) \n",
    "x_test = x_test.drop('dfy', axis=1)\n",
    "x_train = x_train.drop('b/m', axis=1) \n",
    "x_test = x_test.drop('b/m', axis=1)\n",
    "x_train = x_train.drop('dy', axis=1) \n",
    "x_test = x_test.drop('dy', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.drop('dp', axis=1) \n",
    "x_test = x_test.drop('dp', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2RnyIzvBqu-"
   },
   "source": [
    "## Data Transformation\n",
    "The evaluation of the data revealed a high standard deviation in certain features. To mitigate the potential impact of this variability and ensure features contribute equally to model training, data standardization was applied. This process transforms the data to have zero mean and unit variance, effectively balancing the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zuym9T2zMWe_"
   },
   "source": [
    "### Data Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "id": "ZGXWAeX98RJs",
    "outputId": "5b292b78-c5da-40e4-cba2-04f96e5a6106"
   },
   "outputs": [],
   "source": [
    "scaler_x = StandardScaler()\n",
    "columns = x_train.columns\n",
    "\n",
    "x_train[columns] = scaler_x.fit_transform(x_train[columns])  # Fit on x_train, transform x_train\n",
    "x_test[columns] = scaler_x.transform(x_test[columns])        # Transform x_test using the same scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWUN4EI6covO"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDx-fGtFcsKU"
   },
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JBI1q5Uy1f_"
   },
   "source": [
    "### OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G1_vwbYOc9k-",
    "outputId": "2088e418-6725-4685-c257-e9b1c486ed6f"
   },
   "outputs": [],
   "source": [
    "ols = LinearRegression()\n",
    "ols = ols.fit(x_train, y_train)\n",
    "y_insample_pred_ols = ols.predict(x_train)\n",
    "y_outsample_pred_ols = ols.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysing OLS Coeeficient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "68AHbETqI2cT",
    "outputId": "1624b50d-c9f6-4a1a-98e6-3f5b1842e240"
   },
   "outputs": [],
   "source": [
    "# Assuming 'ols' is the fitted Linear Regression model from your code\n",
    "# Access coefficients\n",
    "coefficients = ols.coef_\n",
    "\n",
    "# Access feature names\n",
    "feature_names = x_train.columns\n",
    "\n",
    "# Create a DataFrame for coefficients and their importance\n",
    "coefficients_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients[0]})\n",
    "\n",
    "# Sort by absolute value of coefficients (importance)\n",
    "coefficients_df['Abs_Coefficient'] = np.abs(coefficients_df['Coefficient'])\n",
    "coefficients_df = coefficients_df.sort_values(by='Abs_Coefficient', ascending=False)\n",
    "coefficients_df = coefficients_df.drop(columns=['Abs_Coefficient'])\n",
    "\n",
    "# Display the coefficients and their importance\n",
    "coefficients_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iV-0SwMczJ46"
   },
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time serires Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "tlDbUQKRgKP3"
   },
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=5, test_size=12 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Alpha Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "collapsed": true,
    "id": "xCG2ckgWrfEE",
    "outputId": "d46a15fb-256b-4e32-ab99-330da546e8c0"
   },
   "outputs": [],
   "source": [
    "alphas = np.logspace(-4, 4, 100)\n",
    "ridge_cv = RidgeCV(alphas=alphas, cv=tscv, scoring='neg_mean_squared_error')\n",
    "ridge_cv.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "id": "YhbuD2dOQVK4"
   },
   "outputs": [],
   "source": [
    "ridge = Ridge(alpha=ridge_cv.alpha_,max_iter=10000,fit_intercept=False)\n",
    "ridge.fit(x_train, y_train)\n",
    "y_insample_pred_ridge = ridge.predict(x_train)\n",
    "y_outsample_pred_ridge = ridge.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=5, test_size=12)\n",
    "alphas = np.logspace(1, 1, 100) \n",
    "lasso_cv = LassoCV(alphas=alphas, cv=tscv)\n",
    "lasso_cv.fit(x_train, y_train)\n",
    "# @ Analysing the coefficent\n",
    "coefficients = lasso_cv.coef_\n",
    "\n",
    "feature_names = x_train.columns\n",
    "dropped_features = feature_names[np.where(coefficients ==0)]\n",
    "print(dropped_features)\n",
    "import numpy as np\n",
    "\n",
    "importance = np.abs(coefficients)\n",
    "sorted_indices = np.argsort(importance)[::-1]  # Indices sorted by importance\n",
    "\n",
    "# If you have feature names (e.g., from a pandas DataFrame):\n",
    "for i in sorted_indices:\n",
    "    print(f\"{feature_names[i]}: {importance[i]}\")\n",
    "lasso = Lasso(alpha=lasso_cv.alpha_,max_iter=1000,fit_intercept=False)\n",
    "lasso.fit(x_train, y_train)\n",
    "y_insample_pred_lasso = lasso.predict(x_train)\n",
    "y_outsample_pred_lasso = lasso.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Tree based models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3C63SG159yrb"
   },
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"y_train_flat shape:\", y_train_flat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_VD2jDPEc-QV",
    "outputId": "a72ff022-e85d-4c71-dfc1-e5dc322385c4"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Implement Random Forest with optimized hyperparameter tuning\n",
    "rf = RandomForestRegressor(random_state=42, n_jobs=- 4)  # Use all CPU cores\n",
    "\n",
    "rf_params = {\n",
    "    \"n_estimators\": [50, 100, 200, 300],  # Fewer estimators for faster training\n",
    "    \"max_depth\": [None, 10, 20, 30],     # Balanced depth options\n",
    "    \"min_samples_split\": [2, 5, 10, 15], # Tuning sample split criteria\n",
    "}\n",
    "\n",
    "# Use fewer CV folds if speed is critical\n",
    "rf_random = RandomizedSearchCV(\n",
    "    rf, rf_params, cv=TimeSeriesSplit(n_splits=4, test_size=12), scoring=\"neg_mean_squared_error\"\n",
    ")\n",
    "\n",
    "\n",
    "# Ensure y_train is a 1D array\n",
    "y_train_1d = y_train.to_numpy().ravel()\n",
    "\n",
    "# Fit the model\n",
    "rf_random.fit(x_train, y_train_1d)\n",
    "\n",
    "print(\"Best Random Forest Params:\", rf_random.best_params_)\n",
    "\n",
    "# Predict and evaluate Random Forest\n",
    "y_train_pred_rf = rf_random.best_estimator_.predict(x_train)\n",
    "y_test_pred_rf = rf_random.best_estimator_.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVGly4idxfvX"
   },
   "source": [
    "# Performance Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timing strategy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def timing_strategy_evaluation_with_drawdown(trained_model, X_test, actual_returns, risk_free_rate=0.02 / 12, threshold=0, initial_value=100):\n",
    "    \"\"\"\n",
    "    Evaluate a timing strategy based on a trained model's predictions.\n",
    "\n",
    "    Parameters:\n",
    "        trained_model: Trained machine learning model with a `predict` method.\n",
    "        X_test: DataFrame or array of predictors for testing (features for prediction).\n",
    "        actual_returns: Series or array of actual returns for the evaluation period.\n",
    "        risk_free_rate: Monthly risk-free rate, default is 0.02 annualized.\n",
    "        threshold: Threshold for deciding risk-on or risk-off, default is 0.\n",
    "        initial_value: Initial portfolio value, default is 100.\n",
    "\n",
    "    Returns:\n",
    "        portfolio_values: Series of portfolio values over time.\n",
    "        cumulative_return: Final cumulative return of the portfolio.\n",
    "        sharpe_ratio: Sharpe ratio of the portfolio strategy.\n",
    "        max_drawdown: Maximum drawdown of the portfolio.\n",
    "    \"\"\"\n",
    "    # Predict returns using the trained model\n",
    "    predicted_returns = trained_model.predict(X_test)\n",
    "    \n",
    "    # Ensure actual_returns is a NumPy array for consistency\n",
    "    if isinstance(actual_returns, pd.Series) or isinstance(actual_returns, pd.DataFrame):\n",
    "        actual_returns = actual_returns.values.flatten()\n",
    "    elif not isinstance(actual_returns, (list, tuple)):\n",
    "        raise TypeError(\"actual_returns must be a Series, DataFrame, list, or tuple.\")\n",
    "\n",
    "    # Initialize portfolio for timing strategy\n",
    "    portfolio_values_timing = [initial_value]\n",
    "\n",
    "    # Timing strategy\n",
    "    for i in range(len(predicted_returns)):\n",
    "        if predicted_returns[i] > threshold:  # Risk-On\n",
    "            portfolio_values_timing.append(portfolio_values_timing[-1] * (1 + actual_returns[i]))\n",
    "        else:  # Risk-Off\n",
    "            portfolio_values_timing.append(portfolio_values_timing[-1] * (1 + risk_free_rate))\n",
    "\n",
    "    # Convert portfolio values to pandas Series for analysis\n",
    "    portfolio_values_timing = pd.Series(portfolio_values_timing)\n",
    "\n",
    "    # Calculate performance metrics for timing strategy\n",
    "    cumulative_return_timing = portfolio_values_timing.iloc[-1] / portfolio_values_timing.iloc[0] - 1\n",
    "    sharpe_ratio_timing = (portfolio_values_timing.pct_change().mean() - risk_free_rate) / portfolio_values_timing.pct_change().std()\n",
    "\n",
    "    # Calculate maximum drawdown for timing strategy\n",
    "    rolling_max_timing = portfolio_values_timing.cummax()\n",
    "    drawdown_timing = (portfolio_values_timing - rolling_max_timing) / rolling_max_timing\n",
    "    max_drawdown_timing = drawdown_timing.min()\n",
    "\n",
    "    # Buy-and-hold strategy\n",
    "    portfolio_values_bh = [initial_value]\n",
    "    for ret in actual_returns:\n",
    "        portfolio_values_bh.append(portfolio_values_bh[-1] * (1 + ret))\n",
    "    portfolio_values_bh = pd.Series(portfolio_values_bh)\n",
    "\n",
    "    # Plot portfolio evolution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(portfolio_values_timing, label=\"Timing Strategy\", marker='o', linestyle='-')\n",
    "    plt.plot(portfolio_values_bh, label=\"Buy-and-Hold Strategy\", marker='x', linestyle='--')\n",
    "    plt.title(\"Portfolio Evolution: Timing Strategy vs Buy-and-Hold\")\n",
    "    plt.xlabel(\"Time (Months)\")\n",
    "    plt.ylabel(\"Portfolio Value\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    return portfolio_values_timing, cumulative_return_timing, sharpe_ratio_timing, max_drawdown_timing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Performance Comparism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In-sample Performance Comparism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_insample_ols = mean_squared_error(y_train, y_insample_pred_ols)\n",
    "r2_insample_ols = r2_score(y_train, y_insample_pred_ols)\n",
    "mse_insample_ridge = mean_squared_error(y_train, y_insample_pred_ridge)\n",
    "r2_insample_ridge = r2_score(y_train, y_insample_pred_ridge)\n",
    "mse_insample_lasso = mean_squared_error(y_train, y_insample_pred_lasso)\n",
    "r2_insample_lasso = r2_score(y_train, y_insample_pred_lasso)\n",
    "print('Model Insample Performance Comparison:')\n",
    "print(f'OLS MSE: {mse_insample_ols:.4f}, R-squared: {r2_insample_ols:.4f}')\n",
    "print(f'Ridge MSE: {mse_insample_ridge:.4f}, R-squared: {r2_insample_ridge:.4f}')\n",
    "print(f'Lasso MSE: {mse_insample_lasso:.4f}, R-squared: {r2_insample_lasso:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### out-sample Performance Comparism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_outsample_ols = mean_squared_error(y_test, y_outsample_pred_ols)\n",
    "r2_outsample_ols = r2_score(y_test, y_outsample_pred_ols)\n",
    "mse_outsample_ridge = mean_squared_error(y_test, y_outsample_pred_ridge)\n",
    "r2_outsample_ridge = r2_score(y_test, y_outsample_pred_ridge)\n",
    "mse_outsample_lasso = mean_squared_error(y_test, y_outsample_pred_lasso)\n",
    "r2_outsample_lasso = r2_score(y_test, y_outsample_pred_lasso)\n",
    "print('Model Outsample Performance Comparison:')\n",
    "print(f'OLS MSE: {mse_outsample_ols:.4f}, R-squared: {r2_outsample_ols:.4f}')\n",
    "print(f'Ridge MSE: {mse_outsample_ridge:.4f}, R-squared: {r2_outsample_ridge:.4f}')\n",
    "print(f'Lasso MSE: {mse_outsample_lasso:.4f}, R-squared: {r2_outsample_lasso:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timing Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Timing Strategy for OLS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_values, cumulative_return, sharpe_ratio, max_drawdown = timing_strategy_evaluation_with_drawdown(ols, x_test, y_test)\n",
    "\n",
    "# Display results\n",
    "print(\"Cumulative Return:\", round(cumulative_return * 100, 2), \"%\")\n",
    "print(\"Sharpe Ratio:\", round(sharpe_ratio, 2))\n",
    "print(\"Maximum Drawdown:\", round(max_drawdown * 100, 2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Timing Strategy for Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_values, cumulative_return, sharpe_ratio, max_drawdown = timing_strategy_evaluation_with_drawdown(ridge, x_test, y_test)\n",
    "\n",
    "# Display results\n",
    "print(\"Cumulative Return:\", round(cumulative_return * 100, 2), \"%\")\n",
    "print(\"Sharpe Ratio:\", round(sharpe_ratio, 2))\n",
    "print(\"Maximum Drawdown:\", round(max_drawdown * 100, 2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Timing Strategy for Lasso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_values, cumulative_return, sharpe_ratio, max_drawdown = timing_strategy_evaluation_with_drawdown(lasso, x_test, y_test)\n",
    "\n",
    "# Display results\n",
    "print(\"Cumulative Return:\", round(cumulative_return * 100, 2), \"%\")\n",
    "print(\"Sharpe Ratio:\", round(sharpe_ratio, 2))\n",
    "print(\"Maximum Drawdown:\", round(max_drawdown * 100, 2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Performance comparism "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### insample Performance comparism "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_train_rf = mean_squared_error(y_train, y_train_pred_rf)\n",
    "r2_train_rf = r2_score(y_train, y_train_pred_rf)\n",
    "print(f\"Random Forest - Training MSE: {mse_train_rf:.4f}, Training R²: {r2_train_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### out-sample Performance comparism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_test_rf = mean_squared_error(y_test, y_test_pred_rf)\n",
    "r2_test_rf = r2_score(y_test, y_test_pred_rf)\n",
    "print(f\"Random Forest - Testing MSE: {mse_test_rf:.4f}, Testing R²: {r2_test_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timing Strategy for Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_values, cumulative_return, sharpe_ratio, max_drawdown = timing_strategy_evaluation_with_drawdown(rf_random, x_test, y_test)\n",
    "\n",
    "# Display results\n",
    "print(\"Cumulative Return:\", round(cumulative_return * 100, 2), \"%\")\n",
    "print(\"Sharpe Ratio:\", round(sharpe_ratio, 2))\n",
    "print(\"Maximum Drawdown:\", round(max_drawdown * 100, 2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot for Linear Regression   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fB3CUMiqeYVx",
    "outputId": "7a2e1975-a848-4bdd-ddba-d0c60ac2aeff"
   },
   "outputs": [],
   "source": [
    "# In-sample Predictions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(y_train.index, y_train, label=\"Actual (Train)\", color=\"blue\")\n",
    "plt.plot(y_train.index, y_insample_pred_ridge, label=\"Predicted (Train)\", color=\"orange\")\n",
    "plt.title(\"In-sample Predictions\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"R\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Out-of-sample Predictions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(y_test.index, y_test, label=\"Actual (Test)\", color=\"blue\")\n",
    "plt.plot(y_test.index, y_outsample_pred_ridge, label=\"Predicted (Test)\", color=\"green\")\n",
    "plt.title(\"Out-of-sample Predictions\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"R\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot for Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "pQLb7AQRD1dL",
    "outputId": "8684a5a2-a2e9-486c-d384-180d8c0bf91f"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# In-sample plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(y_train.index, y_train, label='Actual', color='blue')\n",
    "plt.plot(y_train.index, y_train_pred_rf, label='Predicted', color='red')\n",
    "plt.title('Random Forest - In-sample Predictions')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('R')\n",
    "plt.legend()\n",
    "\n",
    "# Out-of-sample plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(y_test.index, y_test, label='Actual', color='blue')\n",
    "plt.plot(y_test.index, y_test_pred_rf, label='Predicted', color='green')\n",
    "plt.title('Random Forest - Out-of-sample Predictions')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('R')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "coRJ5o6CnLLO"
   },
   "source": [
    "# Financial Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBcxkeVfaykn"
   },
   "source": [
    "## Analysing data of Know Historical events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_qIvq-TxBBI3"
   },
   "source": [
    "## Market Valuation Signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividend to Price Ratio vs. Book to Market Ratio\n",
    "The Graph shows strong correlation between the book to market ratio and Divends to price ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N7FHfDIWWAFv",
    "outputId": "4a6850c5-3a59-4be1-9257-1523f8389130"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(DF_FINAL['dp'], DF_FINAL['b/m'], c=DF_FINAL['tms'], cmap='viridis')\n",
    "plt.xlabel('Dividend to Price Ratio (dp)')\n",
    "plt.ylabel('Book to Market Ratio (b/m)')\n",
    "plt.title('Dividend to Price Ratio vs. Book to Market Ratio')\n",
    "_ = plt.colorbar(label='Market Risk Premium (tms)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividend price vs Dividend Yield\n",
    "Dividend price vs Dividend Yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "RRzOrDLoWmg_"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(DF_FINAL['dp'], DF_FINAL['dy'], c=df['tms'], cmap='viridis')\n",
    "plt.xlabel('Dividend to Price Ratio (dp)')\n",
    "plt.ylabel('Dividend to Yields (d/y)')\n",
    "plt.title('Dividend to Price Ratio vs. Dividend to Yields ')\n",
    "_ = plt.colorbar(label='Market Risk Premium (tms)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tm7KcqDw9M_C"
   },
   "source": [
    "##  Spike Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zK5JKA0-9W4i"
   },
   "outputs": [],
   "source": [
    "# Assuming 'R' column represents returns and the index is a datetime index.\n",
    "def find_spike_periods(df, return_column='R', threshold=2):\n",
    "    \"\"\"\n",
    "    Finds periods of spikes in returns exceeding a given threshold.\n",
    "\n",
    "    Args:\n",
    "        DF_FINAL: DataFrame with a datetime index and a return column.\n",
    "        return_column: The name of the column containing returns.\n",
    "        threshold: The standard deviation threshold to identify a spike.\n",
    "\n",
    "    Returns:\n",
    "        A list of tuples, where each tuple represents a spike period\n",
    "        (start_date, end_date).\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate rolling standard deviation to identify volatility\n",
    "    rolling_std = DF_FINAL[return_column].rolling(window=12).std() # Adjust window size as needed\n",
    "\n",
    "    # Identify spikes based on threshold\n",
    "    spikes = DF_FINAL[return_column][rolling_std > threshold * rolling_std.mean()]\n",
    "\n",
    "    # Group consecutive spikes into periods\n",
    "    spike_periods = []\n",
    "    start_date = None\n",
    "    for date in spikes.index:\n",
    "        if start_date is None:\n",
    "            start_date = date\n",
    "        elif date != spikes.index[spikes.index.get_loc(date) - 1] + pd.DateOffset(months=1): # Adjust for your data freq\n",
    "            spike_periods.append((start_date, spikes.index[spikes.index.get_loc(date) - 1]))\n",
    "            start_date = date\n",
    "    if start_date is not None:\n",
    "        spike_periods.append((start_date, spikes.index[-1]))\n",
    "\n",
    "    return spike_periods\n",
    "\n",
    "# Example usage:\n",
    "spike_periods = find_spike_periods(DF_FINAL)\n",
    "print(spike_periods)\n",
    "\n",
    "# For visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(DF_FINAL['R'], label='Returns')\n",
    "plt.plot(DF_FINAL['R'].rolling(window=6).std(), label='Rolling Std Dev')\n",
    "\n",
    "for start, end in spike_periods:\n",
    "    plt.axvspan(start, end, color='red', alpha=0.3, label='Spike Period' if start==spike_periods[0][0] else '') # Plot each spike as a shaded area\n",
    "plt.legend()\n",
    "plt.title('Return Spikes')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Return')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "EgeOWEzKxPYA",
    "rEOjCULwX4Vb",
    "sIt3PcGrmrb0",
    "s2RnyIzvBqu-",
    "WDx-fGtFcsKU",
    "1JBI1q5Uy1f_",
    "iV-0SwMczJ46",
    "3C63SG159yrb",
    "IVGly4idxfvX",
    "coRJ5o6CnLLO"
   ],
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "introML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
